{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment without social influence\n",
    "\n",
    "#### In this notebook, we estimate alignment in conversations.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymc3.graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-583fa5fd3dc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpymc3\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymc3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mceil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymc3.graph'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm \n",
    "from math import ceil\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import utils\n",
    "from talkpages import WikiCorpusReader, WikiCorpus\n",
    "from alignment import Alignment\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The conversations are taken from a selection of 10 topics from the Controversial TalkPages corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = [\n",
    "          'religion',\n",
    "          'science', \n",
    "          'politics', \n",
    "          'history', \n",
    "          'people',\n",
    "          'philosophy', \n",
    "          'sports',\n",
    "          'linguistics', \n",
    "          'psychiatry',\n",
    "          'environment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To count alignment, we use a selection of marker categories and tokens from the LIWC dictionaries. There is no overlap between any two categories due to some preprocessing (`marker selection.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_CATEGORIES = {'stylistic': [\n",
    "                        'articles',\n",
    "                        'negations',\n",
    "                        'prepositions',\n",
    "                        'numbers',\n",
    "                        'pronouns'\n",
    "                    ], \n",
    "                    'rhetoric': [\n",
    "                        'tentative',   \n",
    "                        'certainty',\n",
    "                        'discrepancy',\n",
    "                        'inclusive',\n",
    "                        'exclusive'\n",
    "                    ],\n",
    "                    'discursive': [\n",
    "                        'causation',\n",
    "                        'insight',\n",
    "                        'inhibition',\n",
    "                        'communication',\n",
    "                        'cognitive process',\n",
    "                        'sensory process',\n",
    "                        'motion'\n",
    "                    ],\n",
    "                    'stance': [\n",
    "                        'optimism',\n",
    "                        'anger',\n",
    "                        'anxiety',\n",
    "                        'sadness',\n",
    "                    ]}\n",
    "\n",
    "\n",
    "# Keep a list of category names for convenience.\n",
    "CATEGORY_LIST = []\n",
    "for cats in META_CATEGORIES.values():\n",
    "    CATEGORY_LIST.extend(cats)\n",
    "\n",
    "    \n",
    "# Load the filtered lists of markers. \n",
    "with open('../../data/liwc/final.dict', 'rb') as f:\n",
    "    MARKER_DICT = pickle.load(f)\n",
    "    \n",
    "    marker_list = []\n",
    "    for markers in MARKER_DICT.values():\n",
    "        marker_list.extend(markers)\n",
    "    MARKER_LIST = list(set(marker_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do we focus on `category-not-word` alignment to exclude cases of lexical repetition ([Doyle & Frank 2016](http://www.aclweb.org/anthology/P16-1050), pp. 531-532) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'category'\n",
    "# MODE = 'cnw'\n",
    "\n",
    "MAX_ITERS = 100000\n",
    "N_SAMPLES = 4000\n",
    "TRACE_SIZE = 1000\n",
    "\n",
    "CAUCHY_ALPHA = -2\n",
    "CAUCHY_BETA = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "religion\n",
      "***************\n",
      "religion, articles\n",
      "******************************\n",
      "digraph {\n",
      "\t\"alignment intercept\" [label=\"alignment intercept ~ Normal\"]\n",
      "\t\"baseline intercept\" [label=\"baseline intercept ~ Cauchy\"]\n",
      "\t\"guessing coefficient\" [label=\"guessing coefficient ~ Beta\"]\n",
      "\tsubgraph \"cluster23,223\" {\n",
      "\t\tC_base [label=\"C_base ~ Binomial\" style=filled]\n",
      "\t\tC_align [label=\"C_align ~ Binomial\" style=filled]\n",
      "\t\tlabel=\"23,223\" labeljust=r labelloc=b style=rounded\n",
      "\t}\n",
      "\t\"baseline intercept\" -> C_base\n",
      "\t\"guessing coefficient\" -> C_base\n",
      "\t\"alignment intercept\" -> C_align\n",
      "\t\"baseline intercept\" -> C_align\n",
      "\t\"guessing coefficient\" -> C_align\n",
      "}\n",
      "science\n",
      "***************\n",
      "science, articles\n",
      "******************************\n",
      "digraph {\n",
      "\t\"alignment intercept\" [label=\"alignment intercept ~ Normal\"]\n",
      "\t\"baseline intercept\" [label=\"baseline intercept ~ Cauchy\"]\n",
      "\t\"guessing coefficient\" [label=\"guessing coefficient ~ Beta\"]\n",
      "\tsubgraph \"cluster35,401\" {\n",
      "\t\tC_base [label=\"C_base ~ Binomial\" style=filled]\n",
      "\t\tC_align [label=\"C_align ~ Binomial\" style=filled]\n",
      "\t\tlabel=\"35,401\" labeljust=r labelloc=b style=rounded\n",
      "\t}\n",
      "\t\"baseline intercept\" -> C_base\n",
      "\t\"guessing coefficient\" -> C_base\n",
      "\t\"alignment intercept\" -> C_align\n",
      "\t\"baseline intercept\" -> C_align\n",
      "\t\"guessing coefficient\" -> C_align\n",
      "}\n",
      "politics\n",
      "***************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d71f678e429c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Load category-not-word alignment counts (Doyle & Frank, 2016)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./counts-{}/{}.dill'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOPIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mN_base_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_align_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_base_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_align_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/virtualenvs/align/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, ignore)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;31m# apply kwd settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mpik\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpik\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_main_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/virtualenvs/align/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'__builtin__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_main\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m \u001b[0;31m#XXX: above set w/save_module_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for TOPIC in TOPICS:\n",
    "    \n",
    "    print('{}\\n{}'.format(TOPIC, '*'*15))\n",
    "    \n",
    "    # Load category-not-word alignment counts (Doyle & Frank, 2016)\n",
    "    with open('./counts-{}/{}.dill'.format(MODE, TOPIC), 'rb') as f:\n",
    "        N_base_all, N_align_all, C_base_all, C_align_all, _, _, _, _ = dill.load(f)\n",
    "\n",
    "    \n",
    "    # Statistical modelling\n",
    "    for c, category in enumerate(CATEGORY_LIST):\n",
    "        \n",
    "        print('{}, {}\\n{}'.format(TOPIC, category, '*'*30))\n",
    "        \n",
    "        # Data: collect the counts for this category of markers\n",
    "        N_base, N_align, C_base, C_align = [], [], [], []\n",
    "        for dyad in N_base_all:\n",
    "            if C_base_all[dyad][c] > N_base_all[dyad][c]:\n",
    "                continue\n",
    "            if C_align_all[dyad][c] > N_align_all[dyad][c]:\n",
    "                continue\n",
    "            N_base.append(N_base_all[dyad][c])\n",
    "            C_base.append(C_base_all[dyad][c])\n",
    "            N_align.append(N_align_all[dyad][c])\n",
    "            C_align.append(C_align_all[dyad][c])\n",
    "    \n",
    "    \n",
    "        if not any(N_base):\n",
    "            print('N_base: all zeros.')\n",
    "        if not any(N_align):\n",
    "            print('N_align: all zeros.')\n",
    "        if not any(C_align):\n",
    "            print('C_align: all zeros.')\n",
    "        if not any(C_base):\n",
    "            print('C_base: all zeros.')\n",
    "        \n",
    "        if not (any(C_base) and any(C_align)):\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "        # A simple logistic model.\n",
    "        with pm.Model() as model:\n",
    "            # Parameters\n",
    "            beta0 = pm.Cauchy('baseline intercept', alpha=CAUCHY_ALPHA, beta=CAUCHY_BETA)\n",
    "            alpha0 = pm.Normal('alignment intercept', mu=0, sd=0.05)\n",
    "            \n",
    "            # Include a guessing coefficient for robust logistic regression\n",
    "            # (cfr. J. Kruschke, 2014, 'Doing Bayesian data analysis', pp. 635-636)\n",
    "            guess = pm.Beta('guessing coefficient', alpha=1, beta=9)  \n",
    "            \n",
    "            # Transformed parameters\n",
    "            mu_base  = guess * (1/2) + (1-guess) * pm.math.invlogit(beta0)            \n",
    "            mu_align = guess * (1/2) + (1-guess) * pm.math.invlogit(beta0 + alpha0)\n",
    "#             mu_base  = pm.math.invlogit(beta0)            \n",
    "#             mu_align = pm.math.invlogit(beta0 + alpha0)\n",
    "            \n",
    "            # Model\n",
    "            base_count  = pm.Binomial('C_base' , p=mu_base , observed=C_base, n=N_base)\n",
    "            align_count = pm.Binomial('C_align', p=mu_align, observed=C_align, n=N_align)\n",
    "        \n",
    "\n",
    "        # Inference\n",
    "        with model:\n",
    "            \n",
    "            \n",
    "            print(pm.model_to_graphviz(model))\n",
    "            break\n",
    "            \n",
    "            print(model.check_test_point())\n",
    "    \n",
    "            approx = pm.fit(n=MAX_ITERS, method='advi', \n",
    "                            callbacks=[pm.callbacks.CheckParametersConvergence(diff='absolute')])\n",
    "        \n",
    "            \n",
    "            print('Sampling {} ...'.format(N_SAMPLES), end=' ')\n",
    "            full_trace = approx.sample(draws=N_SAMPLES)\n",
    "            print('Done.')\n",
    "            \n",
    "            trace = full_trace[-TRACE_SIZE:]\n",
    "            trace_df = pm.trace_to_dataframe(trace)\n",
    "#             trace_df.to_csv('./traces/{}/swam/{}-{}.csv'.format(MODE, TOPIC, category))\n",
    "\n",
    "\n",
    "#             print(pm.summary(trace))\n",
    "\n",
    "            \n",
    "            pm.traceplot(trace, varnames=['baseline intercept', 'alignment intercept', 'guessing coefficient'])\n",
    "#             plt.savefig('plots/traceplots/{}/swam/{}-{}.pdf'.format(MODE, TOPIC, category))\n",
    "#             plt.show()\n",
    "\n",
    "            pm.plot_posterior(trace)\n",
    "#             plt.savefig('plots/posteriors/{}/swam/{}-{}.pdf'.format(MODE, TOPIC, category))\n",
    "            plt.show();\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for TOPIC in TOPICS:\n",
    "    \n",
    "    print('{}\\n{}'.format(TOPIC, '*'*15))\n",
    "    \n",
    "    # Load dataframes with precomputed marker counts\n",
    "    csv_filename = './with_counts/{}_fullcounts.csv'.format(TOPIC)\n",
    "    corpus = WikiCorpus(csv_filename)\n",
    "    \n",
    "    # Obtain dataframe of conversational turns\n",
    "    turns = corpus.reply_pairs()\n",
    "    \n",
    "    # Initialise alignment tracker\n",
    "    al = Alignment(corpus, markers)\n",
    "    \n",
    "    # Compute category-not-word alignment counts (Doyle & Frank, 2016)\n",
    "    N_base_all, N_align_all, C_base_all, C_align_all = al.counts(mode='category-not-word')\n",
    "    \n",
    "    \n",
    "    # Statistical modelling\n",
    "    for c, category in enumerate(CATEGORY_LIST):\n",
    "        \n",
    "        print('{}, {}\\n{}'.format(TOPIC, category, '*'*30))\n",
    "        \n",
    "        # Data: collect the counts for this category of markers\n",
    "        N_base, N_align, C_base, C_align = [], [], [], []\n",
    "        for dyad in N_base_all:\n",
    "            N_base.append(N_base_all[dyad][c])\n",
    "            C_base.append(C_base_all[dyad][c])\n",
    "            N_align.append(N_align_all[dyad][c])\n",
    "            C_align.append(C_align_all[dyad][c])\n",
    "    \n",
    "    \n",
    "        # Transformed data\n",
    "        N_base = utils.standardise(N_base)\n",
    "        C_base = utils.standardise(C_base)\n",
    "        N_align = utils.standardise(N_align)\n",
    "        C_align = utils.standardise(C_align)\n",
    "        \n",
    "        \n",
    "        # A simple logistic model.\n",
    "        with pm.Model() as model:\n",
    "            # Parameters\n",
    "            beta0 = pm.Cauchy('baseline intercept', alpha=0, beta=2.5)\n",
    "            alpha0 = pm.Normal('alignment intercept', mu=0, sd=0.25)\n",
    "            \n",
    "            # Include a guessing coefficient for robust logistic regression\n",
    "            # (cfr. J. Kruschke, 2014, 'Doing Bayesian data analysis', pp. 635-636)\n",
    "            guess = pm.Beta(alpha=1, beta=9)  \n",
    "            \n",
    "            # Transformed parameters\n",
    "            mu_base  = guess * 0.5 + (1-guess) * pm.math.invlogit(beta0)            \n",
    "            mu_align = guess * 0.5 + (1-guess) * pm.math.invlogit(beta0 + alpha0)\n",
    "            \n",
    "            # Model\n",
    "            base_count  = pm.Binomial('C_base' , p=mu_base , observed=C_base, n=N_base)\n",
    "            align_count = pm.Binomial('C_align', p=mu_align, observed=C_align, n=N_align)\n",
    "        \n",
    "\n",
    "        # Inference\n",
    "        with individual_model:\n",
    "            start = pm.find_MAP()\n",
    "            step = pm.NUTS(scaling=start)\n",
    "            \n",
    "            out_db = pm.backends.Text('./traces-swam/{}-{}'.format(TOPIC, category))\n",
    "            \n",
    "            trace = pm.sample(draws=2000, \n",
    "                              random_seed=13,\n",
    "                              progressbar=True,\n",
    "                              tune=500,\n",
    "                              chains=4,\n",
    "                              trace=out_db)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
