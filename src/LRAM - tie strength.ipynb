{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment and centrality\n",
    "\n",
    "#### In this notebook, we estimate alignment in conversations and quantify to which amount alignment  is influenced by the centrality of the interlocutors.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import utils\n",
    "from talkpages import WikiCorpusReader, WikiCorpus\n",
    "from alignment import Alignment\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The conversations are taken from a selection of 10 topics from the Controversial TalkPages corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = ['religion',\n",
    "          'science', \n",
    "          'politics', \n",
    "          'history', \n",
    "          'people',\n",
    "          'philosophy', \n",
    "          'sports',\n",
    "          'linguistics', \n",
    "          'psychiatry',\n",
    "          'environment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To count alignment, we use a selection of marker categories and tokens from the LIWC dictionaries. There is no overlap between any two categories due to some preprocessing (`marker selection.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_CATEGORIES = {'stylistic': [\n",
    "                        'articles',\n",
    "                        'negations',\n",
    "                        'prepositions',\n",
    "                        'numbers',\n",
    "                        'pronouns'\n",
    "                    ], \n",
    "                    'rhetoric': [\n",
    "                        'tentative',   \n",
    "                        'certainty',\n",
    "                        'discrepancy',\n",
    "                        'inclusive',\n",
    "                        'exclusive'\n",
    "                    ],\n",
    "                    'discursive': [\n",
    "                        'causation',\n",
    "                        'insight',\n",
    "                        'inhibition',\n",
    "                        'communication',\n",
    "                        'cognitive process',\n",
    "                        'sensory process',\n",
    "                        'motion'\n",
    "                    ],\n",
    "                    'stance': [\n",
    "                        'optimism',\n",
    "                        'anger',\n",
    "                        'anxiety',\n",
    "                        'sadness'\n",
    "                    ]}\n",
    "\n",
    "\n",
    "# Keep a list of category names for convenience.\n",
    "CATEGORY_LIST = []\n",
    "for cats in META_CATEGORIES.values():\n",
    "    CATEGORY_LIST.extend(cats)\n",
    "\n",
    "    \n",
    "# Load the filtered lists of markers. \n",
    "with open('../../data/liwc/final.dict', 'rb') as f:\n",
    "    MARKER_DICT = pickle.load(f)\n",
    "    \n",
    "    marker_list = []\n",
    "    for markers in MARKER_DICT.values():\n",
    "        marker_list.extend(markers)\n",
    "    MARKER_LIST = list(set(marker_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We focus on `category-not-word` alignment to exclude cases of lexical repetition ([Doyle & Frank 2016](http://www.aclweb.org/anthology/P16-1050), pp. 531-532)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'category'\n",
    "# MODE = 'cnw'\n",
    "\n",
    "MAX_ITERS = 100000\n",
    "N_SAMPLES = 4000\n",
    "TRACE_SIZE = 1000\n",
    "\n",
    "CAUCHY_ALPHA = -2\n",
    "CAUCHY_BETA = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "religion\n",
      "***************\n",
      "religion, articles\n",
      "******************************\n",
      "baseline intercept                    -2.06\n",
      "alignment intercept                    0.47\n",
      "coefficient tie-strength              -0.92\n",
      "guessing coefficient_logodds__        -1.05\n",
      "C_base                             -3596.19\n",
      "C_align                          -494324.09\n",
      "Name: Log-probability of test_point, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 85,135: 100%|██████████| 100000/100000 [07:01<00:00, 237.32it/s]   \n",
      "Finished [100%]: Average Loss = 85,135\n",
      "INFO:pymc3.variational.inference:Finished [100%]: Average Loss = 85,135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 4000 ... Done.\n",
      "                              mean        sd  mc_error   hpd_2.5  hpd_97.5\n",
      "baseline intercept       -4.563773  0.034286  0.001008 -4.626820 -4.495294\n",
      "alignment intercept       0.454950  0.036409  0.001065  0.383105  0.524691\n",
      "coefficient tie-strength  0.024793  0.001815  0.000051  0.021389  0.028443\n",
      "guessing coefficient      0.623259  0.000438  0.000013  0.622442  0.624163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mario/GitHub/virtualenvs/align/lib/python3.7/site-packages/matplotlib/axes/_base.py:3604: MatplotlibDeprecationWarning: \n",
      "The `ymin` argument was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use `bottom` instead.\n",
      "  alternative='`bottom`', obj_type='argument')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "religion, negations\n",
      "******************************\n",
      "baseline intercept                     -2.06\n",
      "alignment intercept                     0.47\n",
      "coefficient tie-strength               -0.92\n",
      "guessing coefficient_logodds__         -1.05\n",
      "C_base                           -4168890.04\n",
      "C_align                            -42492.48\n",
      "Name: Log-probability of test_point, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 11,935:  42%|████▏     | 41897/100000 [02:37<04:20, 222.64it/s]    \n",
      "Interrupted at 41,903 [41%]: Average Loss = 4.5626e+05\n",
      "INFO:pymc3.variational.inference:Interrupted at 41,903 [41%]: Average Loss = 4.5626e+05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 4000 ... Done.\n",
      "                              mean        sd      mc_error   hpd_2.5  hpd_97.5\n",
      "baseline intercept       -7.653564  0.021411  7.293027e-04 -7.695549 -7.615448\n",
      "alignment intercept       1.431192  0.102633  3.498371e-03  1.243548  1.634716\n",
      "coefficient tie-strength -0.166236  0.066985  2.072410e-03 -0.294051 -0.048959\n",
      "guessing coefficient      0.000070  0.000018  5.392273e-07  0.000038  0.000106\n",
      "religion, prepositions\n",
      "******************************\n",
      "baseline intercept                     -2.06\n",
      "alignment intercept                     0.47\n",
      "coefficient tie-strength               -0.92\n",
      "guessing coefficient_logodds__         -1.05\n",
      "C_base                           -4173076.59\n",
      "C_align                            -42641.00\n",
      "Name: Log-probability of test_point, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 8,132.5:  51%|█████     | 51012/100000 [03:30<04:05, 199.75it/s]   \n",
      "Interrupted at 51,014 [51%]: Average Loss = 3.7518e+05\n",
      "INFO:pymc3.variational.inference:Interrupted at 51,014 [51%]: Average Loss = 3.7518e+05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 4000 ... "
     ]
    }
   ],
   "source": [
    "for TOPIC in TOPICS:\n",
    "    \n",
    "    print('{}\\n{}'.format(TOPIC, '*'*15))\n",
    "    \n",
    "    # Load category-not-word alignment counts (Doyle & Frank, 2016)\n",
    "    with open('./counts-{}/{}.dill'.format(MODE, TOPIC), 'rb') as f:\n",
    "        N_base_all, N_align_all, C_base_all, C_align_all, _, _, dyad2strength, _ = dill.load(f)\n",
    "    \n",
    "    # Statistical modelling\n",
    "    for c, category in enumerate(CATEGORY_LIST):\n",
    "        \n",
    "        print('{}, {}\\n{}'.format(TOPIC, category, '*'*30))\n",
    "        \n",
    "        # Data\n",
    "        N_base, N_align, C_base, C_align = [], [], [], []\n",
    "        tie_strengths = []\n",
    "        \n",
    "        # collect the counts for this category of markers\n",
    "        for dyad in N_base_all:\n",
    "            if C_base_all[dyad][c] > N_base_all[dyad][c]:\n",
    "                continue\n",
    "            if C_align_all[dyad][c] > N_align_all[dyad][c]:\n",
    "                continue\n",
    "            try:\n",
    "                tie_strengths.append(dyad2strength[dyad])\n",
    "            except KeyError:\n",
    "                continue\n",
    "            N_base.append(N_base_all[dyad][c])\n",
    "            C_base.append(C_base_all[dyad][c])\n",
    "            N_align.append(N_align_all[dyad][c])\n",
    "            C_align.append(C_align_all[dyad][c])\n",
    "        \n",
    "        if not any(N_base):\n",
    "            print('N_base: all zeros.')\n",
    "        if not any(N_align):\n",
    "            print('N_align: all zeros.')\n",
    "        if not any(C_align):\n",
    "            print('C_align: all zeros.')\n",
    "        if not any(C_base):\n",
    "            print('C_base: all zeros.')\n",
    "        \n",
    "        if not (any(N_base) or any(N_align) or any(C_align) or any(C_base)):\n",
    "            continue\n",
    "        \n",
    "        tie_strengths =  utils.standardise(tie_strengths)\n",
    "        \n",
    "        # A simple logistic model\n",
    "        with pm.Model() as model:\n",
    "            # Parameters\n",
    "            beta0 = pm.Cauchy('baseline intercept', alpha=CAUCHY_ALPHA, beta=CAUCHY_BETA)\n",
    "            alpha0 = pm.Normal('alignment intercept', mu=0, sd=0.25)\n",
    "            alpha1 = pm.Normal('coefficient tie-strength', mu=0, sd=1)\n",
    "            \n",
    "            # Include a guessing coefficient for robust logistic regression\n",
    "            # (cfr. J. Kruschke, 2014, 'Doing Bayesian data analysis', pp. 635-636)\n",
    "            guess = pm.Beta('guessing coefficient', alpha=1, beta=9)  \n",
    "            \n",
    "            # Transformed parameters\n",
    "            mu_base  = guess * 0.5 + (1-guess) * pm.math.invlogit(beta0)            \n",
    "            mu_align = guess * 0.5 + (1-guess) * pm.math.invlogit(beta0+alpha0 + alpha1*tie_strengths)\n",
    "            \n",
    "            # Model\n",
    "            base_count  = pm.Binomial('C_base' , p=mu_base , observed=C_base, n=N_base)\n",
    "            align_count = pm.Binomial('C_align', p=mu_align, observed=C_align, n=N_align)\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Inference\n",
    "        with model:\n",
    "\n",
    "            print(model.check_test_point())\n",
    "    \n",
    "            approx = pm.fit(n=MAX_ITERS, method='advi', \n",
    "                            callbacks=[pm.callbacks.CheckParametersConvergence(diff='absolute')])\n",
    "\n",
    "            print('Sampling {} ...'.format(N_SAMPLES), end=' ')\n",
    "            full_trace = approx.sample(draws=N_SAMPLES)\n",
    "            print('Done.')\n",
    "            \n",
    "            trace = full_trace[-TRACE_SIZE:]\n",
    "            trace_df = pm.trace_to_dataframe(trace)\n",
    "            trace_df.to_csv('./traces/{}/tiestrength/{}-{}.csv'.format(MODE, TOPIC, category))\n",
    "\n",
    "\n",
    "            print(pm.summary(trace))\n",
    "\n",
    "            \n",
    "            pm.traceplot(trace, varnames=['baseline intercept', \n",
    "                                          'alignment intercept',\n",
    "                                          'coefficient tie-strength',\n",
    "                                          'guessing coefficient'])\n",
    "            \n",
    "            plt.savefig('plots/traceplots/{}/tiestrength/{}-{}.pdf'.format(MODE, TOPIC, category))\n",
    "\n",
    "\n",
    "            pm.plot_posterior(trace)\n",
    "            plt.savefig('plots/posteriors/{}/tiestrength/{}-{}.pdf'.format(MODE, TOPIC, category))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for TOPIC in TOPICS:\n",
    "    \n",
    "    print('{}\\n{}'.format(TOPIC, '*'*15))\n",
    "    \n",
    "    # Load dataframes with precomputed marker counts\n",
    "    csv_filename = './with_counts/{}_fullcounts.csv'.format(TOPIC)\n",
    "    corpus = WikiCorpus(csv_filename)\n",
    "    \n",
    "    # Obtain dataframe of conversational turns\n",
    "    turns = corpus.reply_pairs()\n",
    "    \n",
    "    # Generate network of TalkPages users \n",
    "    # (bool) prune: prune to the largest connected component?\n",
    "    users = corpus.get_users()\n",
    "    net = corpus.social_network(prune=False)\n",
    "    \n",
    "    # Compute centrality for each user and include into the dataframe of reply pairs\n",
    "    corpus.assign_tie_strength()\n",
    "    \n",
    "    # Initialise alignment tracker\n",
    "    al = Alignment(corpus, MARKER_DICT)\n",
    "    \n",
    "    # Compute category-not-word alignment counts (Doyle & Frank, 2016)\n",
    "    N_base_all, N_align_all, C_base_all, C_align_all, _, _, dyad2strength = al.counts(mode='category-not-word',\n",
    "                                                                                all_info=True)\n",
    "    \n",
    "    \n",
    "    # Statistical modelling\n",
    "    for c, category in enumerate(CATEGORY_LIST):\n",
    "        \n",
    "        print('{}, {}\\n{}'.format(TOPIC, category, '*'*30))\n",
    "        \n",
    "        # Data:\n",
    "        # collect the counts for this category of markers\n",
    "        N_base, N_align, C_base, C_align = [], [], [], []\n",
    "        tie_strengths = []\n",
    "        \n",
    "        for dyad in N_base_all:\n",
    "            N_base.append(N_base_all[dyad][c])\n",
    "            C_base.append(C_base_all[dyad][c])\n",
    "            N_align.append(N_align_all[dyad][c])\n",
    "            C_align.append(C_align_all[dyad][c])\n",
    "            \n",
    "            tie_strengths.append(dyad2strength[dyad])\n",
    "    \n",
    "    \n",
    "        # Transformed data\n",
    "        N_base = utils.standardise(N_base)\n",
    "        C_base = utils.standardise(C_base)\n",
    "        N_align = utils.standardise(N_align)\n",
    "        C_align = utils.standardise(C_align)\n",
    "        \n",
    "        \n",
    "        # A simple logistic model.\n",
    "        with pm.Model() as model:\n",
    "            # Parameters\n",
    "            beta0 = pm.Cauchy('baseline intercept', alpha=0, beta=2.5)\n",
    "            alpha0 = pm.Normal('alignment intercept', mu=0, sd=0.25)\n",
    "            alpha1 = pm.Normal('coefficient tie-strength', mu=0, sd=1)\n",
    "            \n",
    "            # Include a guessing coefficient for robust logistic regression\n",
    "            # (cfr. J. Kruschke, 2014, 'Doing Bayesian data analysis', pp. 635-636)\n",
    "            guess = pm.Beta(alpha=1, beta=9)  \n",
    "            \n",
    "            # Transformed parameters\n",
    "            mu_base  = guess * 0.5 + (1-guess) * pm.math.invlogit(beta0)            \n",
    "            mu_align = guess * 0.5 + (1-guess) * pm.math.invlogit(beta0+alpha0 + alpha1*tie_strengths)\n",
    "            \n",
    "            # Model\n",
    "            base_count  = pm.Binomial('C_base' , p=mu_base , observed=C_base, n=N_base)\n",
    "            align_count = pm.Binomial('C_align', p=mu_align, observed=C_align, n=N_align)\n",
    "        \n",
    "\n",
    "        # Inference\n",
    "        with individual_model:\n",
    "            start = pm.find_MAP()\n",
    "            step = pm.NUTS(scaling=start)\n",
    "            \n",
    "            out_db = pm.backends.Text('./traces-tiestrength/{}-{}'.format(TOPIC, category))\n",
    "            \n",
    "            trace = pm.sample(draws=2000, \n",
    "                              random_seed=13,\n",
    "                              progressbar=True,\n",
    "                              tune=500,\n",
    "                              chains=4,\n",
    "                              trace=out_db)\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
